
\section{Introduction} \label{sec:intro}

The steady increase in the number of  cores in today's computers is driving software developers to allow more parallelism.
Indeed, many recent works have developed scalable concurrent data
structures~\cite{ArbelA2014,DrachslerVY2014,NatarajanM2014,BrownER2014,CrainGR2013,BraginskyP2012,
AfekKKMT2012,EllenFRB2010,BronsonCCO2010,HerlihyLLS2007,Michael:1996}.
Such efforts are often very successful, achieving performance that scales well
with the number of threads.
Nevertheless, each of these project generally focuses on a single data structure
(for example, a binary search tree~\cite{ArbelA2014} or a queue~\cite{Michael:1996}) and manually optimizes its implementation.
Proving the correctness of such custom-tailored data structures is painstaking
(for example, the proofs of \cite{BraginskyP2012,EllenFRB2010} are $31$ and $20$ pages long
% \Idit{add info}, 
respectively). We propose an approach to replace this labor-intensive process by automatic means.

One way to automatically convert a sequential data structure into a correct (thread-safe) concurrent one using locks.
The trivial way to do so is to add a single global lock protecting the entire data structure
(as in \emph{synchronized methods} in Java$^{TM}$, for example), but this allows no parallelism whatsoever.
A more sophisticated approach can instrument the code (at compile time) and add
fine-grained lock and unlock instructions (e.g.,~\cite{Gueta2011,MZGB:POPL06}). Such
methods are applicable to certain data structure families, for example,
Domination Locking~\cite{Gueta2011} is applicable to trees or forests
(including binary search trees, BTrees, Treaps~\cite{AragonS1989}, and self adjusting heaps~\cite{Sleator:SAH1986:SAH}),
and employs a variant of hand-over-hand locking~\cite{SilberschatzK1980},
acquiring and releasing locks as it goes down the tree.
%%Other approaches are applicable to DAGS~\cite{dag-locking}.
Unfortunately, to date, solutions of this sort scale poorly~\cite{Gueta2011}.
This is due to synchronization bottlenecks such as the root of the tree,
which is locked by all operations.

In this paper, we circumvent such synchronization bottlenecks via judicious use of optimism.
Specifically, we replace many (but not all) locks with speculative execution and later re-validation.
If re-validation fails, the speculative phase is restarted.
In striking the balance between optimism and pessimism, we exploit the common nature of data structure operations,
which typically begin by traversing the data structure to a designated location, and then perform (mostly local) updates at that location.
Our optimistic execution is limited to the initial read-only part of the code (the data structure traversal)\footnote{Our solution may be seen as a form of software lock elision for read-only operation prefixes.}.
Unlike most software transactional memory approaches~\cite{HLR:SLCA2010},
our synthesized code neither speculatively modifies shared memory contents, nor does it defer writes.
Hence it never needs to rollback, and saves the overhead for tracking writes in dedicated data structures.
Further comparison with related work appears in Section~\ref{sec:related}.

Given a sequential data structure implementation,
our solution first applies a given (black-box) mechanism that instruments the code to add
fine-grained locks that satisfy some \emph{locking protocol}, e.g.,~\cite{Gueta2011,MZGB:POPL06}. 
Our assumptions on the data structure and locking scheme are detailed in Section~\ref{sec:model}.

We then invoke our algorithm, presented in Section~\ref{sec:algorithm}, which
(1) adds version numbers to shared memory objects,
(2) identifies the read-only prefix of each operation,
(3) replaces locks in the read-only prefix with tracking of both locks and versions of read objects, and
(4) introduces appropriate re-validation mechanisms.
Re-validation occurs at the end of the read-only phase, as well as during timeouts and exceptions.
The latter addresses exceptions and infinite loops that may arise when an operation sees an inconsistent view of the data structure.

Our code transformation is general, in the sense that it applies to any data structure for which an appropriate locking protocol exists,
as proven in Section~\ref{sec:proof}. The transformation is trivial to implement at compile time.

We realize our approach with the domination locking scheme \cite{Gueta2011},
which is applicable to tree and forest data structures.
We apply the appropriate code transformations to balanced and unbalanced tree data structure implementations in Java$^{\sc TM}$.
In Section~\ref{sec:eval}, we evaluate the resulting code on a $32$-core machine,
and compare it to fully pessimistic as well as state-of-the-art hand-crafted
data structure implementations~\cite{DrachslerVY2014,BronsonCCO2010}.
Our results show that the introduction of optimistic traversals successfully overcomes synchronization bottlenecks:
the data structures we synthesize scale well with the number of threads,
and achieve comparable performance to that of custom-tailored solutions. 
The benefits of the synthesized code are most pronounced for write-dominated workloads and on large trees.

To conclude, this paper illustrates that automatic synchronization is a promising approach for bringing legacy code to emerging computer architectures.
While this paper illustrates the method for tree data structures, we believe that the general direction may be more broadly applicable.
Section~\ref{sec:discussion} concludes the paper and touches on some directions for future work.