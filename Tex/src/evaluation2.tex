\section{Evaluation}
\label{sec:eval}

In this section we evaluate the performance of our approach on two types of data
structures (1)~supporting only insert delete and get operations (see
Section~\ref{sec:readwrite}), (2)~supporting range queries that retrieve all
records within the range in addition to the basic single record operations (see
Section~\ref{sec:range}). We compare the performance of our approach in terms of
throughput to fully pessimistic approaches, applying fine-grain locking. These
algorithms also serve as the lock-based reference implementation at the base of
our semi-optimistic implementations. To complement the evaluation, we also
compare our approach to hand-crafted state-of-the-art implementations of each
data structure.         

We follow a standard evaluation methodology
(\cite{DrachslerVY2014,NatarajanM2014,BrownER2014,ArbelA2014}). Each experiment
consists of $5$ trials. A trial is a five second run in which each thread continuously executes
randomly chosen operations with respect to the workload distribution. Each trial
begins by initiating a new data structure and applying an untimed pre-filling
phase, which continues until the size of the data structure is within 5\% of
$10^6$ records. The presented results are the average throughput over all trials.    
The keys in each trial and pre-filling phase are selected uniformly at random
from the range $[0,2\cdot10^6]$.
We also experimented with a smaller range ($[0,2\cdot10^4]$) to test different
contention levels; since the results show similar trends they are omitted. 

All implementations are written in Java. We ran the experiments on four Intel
Xeon E5-4650 processors, each with 8 cores for a total of 32 threads 
(with hyper-threading disabled). 
We used Ubuntu 12.04.4 LTS and Java Runtime Environment (build
1.7.0\_51-b13) using the 64-Bit Server VM (build 24.51-b03, mixed mode).

\subsection{Insert-Delete-Get Operations}
\label{sec:readwrite} 

We start by benchmarking search-tree data structure supporting the basic insert
delete and get (lookup) operations. Our experiments evaluate unbalanced as well
as balanced trees.  

We consider textbook sequential implementations of an unbalanced binary
tree, and a treap~\cite{AragonS1989}. To
generate a pessimistic lock-based implementation we manually synthesize
concurrent code by applying the domination locking technique to the sequential
data structures. The resulting algorithms are denoted \domTree and \domTreap.
Finally, we manually apply our lock-removal transformation to the reference
implementations to get our semi-optimistic version of the code, which we call
\autoTree and \autoTreap, respectively.     

We further compare our implementations to their hand-crafted state-of-the-art counterparts. We compare \autoTree to
\begin{itemize}
\item \danaTree The locked-based 
				unbalanced tree of Drachsler et
				al.~\cite{DrachslerVY2014}\footnote{Implementation provided by the authors.}.
\end{itemize}
\autoTreap is evaluated against three hand-crafted implementations
\begin{itemize}
\item \danaAVL The locked-based relaxed balanced AVL tree of 
				Drachsler et al.~\cite{DrachslerVY2014}\footnote{Implementation available at \\
				\texttt{https://github.com/logicalordering/trees}}.
\item \bronson The locked based relaxed balanced AVL tree
				of Bronson et al.~\cite{BronsonCCO2010}\footnote{Implementation available at \\
				\texttt{https://github.com/nbronson/snaptree}}.
\item \skiplist The non-blocking skip-list in the 
				the Java standard library; based on the work of
				Fraser and Harris~\cite{fraser2004practical}.
\end{itemize}

We evaluate performance in three representative workloads distributions:
\emph{read-only} workload comprising of $100\%$ lookup operations, \emph{write-dominated}
workload comprising of insert and delete operations ($50\%$ each), and
\emph{mixed workload} comprising of $50\%$ lookups, $25\%$ inserts and $25\%$
deletes.

Figure~\ref{evaluation:results:unbalanced} 
shows the throughput of unbalanced data structures and Figure~\ref{evaluation:results:balanced} shows
the throughput of the balanced ones. We see that our semi-optimistic
solution is far superior to previous, fully-pessimistic, 
automated approaches, with up-to two order of magnitude higher performance
(between x50 in write dominated workload to x70 in read-only workload). It
successfully overcomes the bottlenecks associated with lock contention, and in
many scenarios comes close to custom-tailored implementations. 

\begin{figure*}
\begin{center}
\input{plots/unbalanced2}
\end{center}
\caption{Throughput of unbalanced data structures.}
\label{evaluation:results:unbalanced}
\end{figure*}


\begin{figure*}
\begin{center}
\input{plots/balanced2}
\end{center}
\caption{Throughput of balanced data
structures.}
\label{evaluation:results:balanced}
\end{figure*}

The results for the read-only workload show the main overhead
of our automatic approach. By profiling the code, we learned 
that the bulk of this overhead stems from the need to track all read objects,
which is inherent to our automatic transformation. 
This is in contrast with the hand-crafted implementations,
which have small overhead on reads in this scenario, thanks to either 
wait-free reads (in \danaTree and \danaAVL), or optimistic validation (in \bronson). 
 
As the ratio of updates in the workload increases, our automatic implementation 
closes this gap.
These results show that our automatic transformation deals well with update contention. 
This might be due to the fact that once
an update phase begins, the operation is not delayed due to concurrent 
read-only operations. 


\subsection{Range Queries}
\label{sec:range} 

This section evaluates the performance of our approach when the data
structure supports a more intricate functionality like range queries. Hand
crafting an implementation of a data structure that supports atomic
(linearizable) range queries is challenging.
The implementations that do support iterating through records may impose an
additional overhead on the regular read and write operations to ensure
progress of range queries.
The results in this section demonstrate that our automatic method
allows generating a correct and efficient code, which is otherwise difficult
to obtain.

We use a skip-list that easily supports range queries by the
nature of its linked-structure. The core of the implementation is the key lookup
method; once reaching the key, a record can be added or be removed in place or
an iteration of subsequent records can be executed by traversing
the bottom level linked-list.

The domination locking scheme cannot be efficiently applied to the skip list
structure since it is a DAG rather than a tree structure. Therefore, we need to
manually devise a pessimistic locking protocol. Our
textbook~\cite{HerlihyS2008} algorithm applies hand-over-hand locking of each
level, such that at the end of the search the operation holds locks on two
records in each level, which define the minimal interval within this level that
contains the lookup key or the range in the case of a range query. 
Once these locks are held any update can be executed in isolation from other
update operations. The operation uses hand-over-hand locking to traverse over
all records within the range. This pessimistic lock-based algorithm
is denoted \domSkiplist. As in previous data structures, we manually apply the
lock-removal method to the reference implementation to get a semi-optimistic algorithm, which we call \autoSkiplist.

\autoSkiplist is compared to \domSkiplist and is further evaluated against two
state-of-the-art data structures which support range queries \bronson and \skiplist.
\bronson provides atomic range queries by traversing a clone of the
original tree that is lazily generated,
whereas iterations in Java's concurrent skip list are not
guaranteed to be atomic.

The evaluation of range queries focuses on a mixed workload where half of the
threads are dedicated to performing range queries, and the other half performs a
mix of insert and delete operations (50\% each). 
\eshcar{what about a workload that includes only 100\% range
queries?} 
We experiment with queries with ranges varying between $10$ to $20$
keys.
% selected uniform at random from $[10,20]$.  
\eshcar{ what about medium/big size scans?}.

We measure the throughput of range queries and update operations separately,
that is, the overall number of range queries executed per time unit - reported
in Figure~\ref{evaluation:results:range}, and the overall number of update (insert and
delete) operations executed per time unit - reported in
Figure~\ref{evaluation:results:update}.

% \begin{figure*}
% \begin{center}
% \input{plots/range}
% \end{center}
% \caption{Throughput of skip-list operations.}
% \label{evaluation:results:range}
% \end{figure*}

\begin{figure*}
	\begin{center}
	\begin{subfigure}[t]{.45\textwidth}
		\caption{Range queries} 
		\input{plots/range}
		\label{evaluation:results:range}
	\end{subfigure}
	\quad\quad\quad
	\begin{subfigure}[t]{.45\textwidth}
		\caption{Insert and delete operations}
		\input{plots/update}
		\label{evaluation:results:update}
	\end{subfigure}
	\ref{skiplistLegened}
	\end{center}
\caption{Mixed workload -- half the threads execute range queries and half the
threads execute insert and delete operations}
\label{evaluation:results:skiplist}
\end{figure*}

These results demonstrate again that our semi-optimistic
solution is far superior to fully-pessimistic fine-grain solution, with two
order of magnitude higher throughput (x75 for range queries and x100 for
update operations). Furthermore, the performance of \autoSkiplist is almost
identical to that of \skiplist, despite the fact that \skiplist performs
inconsistent iterations.

The throughput of \bronson flattens out. For range queries
(Figure~\ref{evaluation:results:range}) the reason might be that this
implementation is
optimized for full scans or very big range queries running sequentially.
The overhead of initiating an epoch per range query---waiting for all
pending update operations to complete---prevents them from scaling when the
ranges are small, and several queries are executed in parallel. 
In addition, the lazy cloning required to support
range queries imposes an overhead on the update operations
(Figure~\ref{evaluation:results:update})---copying each node during downward
traversal---preventing these operations from scaling as well. 
