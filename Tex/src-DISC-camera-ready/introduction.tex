
\section{Introduction} \label{sec:intro}

%\subsection{Generic Lock Removal}
% Parallelizing data strucutres is important for performance
The steady increase in the number of cores in today's computers is driving software developers to allow more and more parallelism.
An important focal point for such efforts is scaling the concurrency of shared data structures, which are often a principal friction point among threads.
%; recent work has illustrated that improved data structure concurrency can lead to
%benefits in overall system performance~\cite{clsm-poster}.
%It is therefore not surprising that many recent works have been dedicated to developing scalable concurrent data
Many recent works have been dedicated to developing scalable concurrent data
structures (e.g.,~\cite{ArbelA2014,DrachslerVY2014,NatarajanM2014,BrownER2014,CrainGR2013,BraginskyP2012,
AfekKKMT2012,EllenFRB2010,BronsonCCO2010,HerlihyLLS2007,fraser2004practical,Michael:1996}),
some of which are widely used in real-world systems~\cite{Ohad:OOPSLA11}.

% They are difficult to build and of resticted use
Each of these projects generally focuses on a single data
structure (e.g., a binary search tree~\cite{BronsonCCO2010} or a queue~\cite{Michael:1996}) and manually optimizes its implementation. These data structures are developed by concurrency experts, typically PhDs or PhD candidates, and
proving their correctness is painstaking;
for example, the proofs of \cite{BraginskyP2012,EllenFRB2010} are $31$ and $20$ pages long,
respectively.
The rationale behind dedicating so much effort to one data structure is that it is
generic and can be used by many applications. Nevertheless,  systems often use data structures in unique ways
that necessitate changing or extending their code (e.g.,~\cite{levelDB,jmonkey,OhadThesis,zyulkyarov2009atomic}), in which cases custom-tailored
implementations may not meet the requirements. 
%Hence, the return-on-investment for such endeavors may be suboptimal.
Here, we propose an approach to facilitate this labor-intensive process,
making scalable synchronization more readily available.

% We give a transformation
Specifically, we present in Section~\ref{sec:algorithm} an algorithm for a source-to-source
code transformation that takes a lock-based concurrent data structure implementation as its input
and generates more scalable code for the same data structure via judicious use of optimism.
%%%Section~\ref{sec:model} details our model and assumptions, and
%%%Section~\ref{sec:algorithm} specifies the transformation.
Our approach combines optimism and pessimism in a practical way.
Like some previous hand-crafted solutions~\cite{LazyList}, 
%%In striking the balance between the two, 
we exploit the common access pattern in data structure operations,
(e.g., tree insertion or deletion), which typically begin by traversing the data structure (to the insertion or deletion point), and then perform local updates at that location.
Our solution replaces locks in the initial read-only traversal with optimistic synchronization, and performs updates using the original lock-based code. It
may thus be seen as a form of software lock elision for read-only prefixes of operations (transactions).
%
% Best of both worlds
Combining optimism and pessimism allows us to achieve ``the best of both worlds'' -- 
while the
optimistic traversal increases concurrency and eliminates bottlenecks,
the use of pessimistic updates saves the overhead associated with speculative or deferred shared
memory updates, (as occurs in \emph{software transactional memory (STM)}~\cite{HLR:SLCA2010}).
%The partially-optimistic execution is compatible with the original code, which permits us to re-execute operations
%pessimistically when too many conflicts occur, avoiding livelocks.
%Furthermore, it allows for code optimizations
%that make the optimistic execution fail in some conflict-free cases (for example, when too many items would have been locked
%by the original code), since we can always fallback upon lock-based execution.
%


% Properties of our transformation
We show in Section~\ref{sec:proof} that our transformation preserves the external behavior (e.g., linearizability, serializability, and deadlock-freedom) of the original lock-based code; formal proofs appear in Appendix~\ref{sec:formal-proof}.
%In other words, if the original code is correct (in the sense of serializability, linearizability, and deadlock-freedom), so is the
%transformed version. 
Moreover, our transformation preserves 
\emph{disjoint access parallelism}~\cite{Israeli:1994:DIS:197917.198079}, (the property that threads 
that access disjoint data objects do not contend on low level shared memory locations), as it
refrains from introducing a shared global clock (as some STM systems do~\cite{DBLP:conf/eurosys/ShalevS06}) or other sources of contention. 


%\subsection{Towards Fully Automatic Parallelization}
% Automatic parallelization
One important use case for our transformation is to apply it in conjunction with automatic lock-based
parallelization mechanisms~\cite{Gueta2011,MZGB:POPL06}.
The latter instrument sequential code 
and add fine-grained locks
% and unlock instructions 
that ensure its safety in concurrent executions.
%For example, \emph{domination locking}~\cite{Gueta2011} is applicable to trees or forests
%(including binary search trees, b-trees, treaps~\cite{AragonS1989}, and self adjusting heaps~\cite{Sleator:SAH1986:SAH}); it
%employs a variant of hand-over-hand locking~\cite{SilberschatzK1980},
%acquiring and releasing locks as it goes down the tree.
%%Other approaches are applicable to DAGS~\cite{dag-locking}.
Our evaluation shows that, by themselves, solutions of this sort may scale poorly.
This is due to synchronization bottlenecks, e.g., the root of a tree,
which is locked by all operations.
By subsequently applying our transformation, one can optimize
the lock-based code they produce, yielding an end-to-end approach to
scalable parallelization of sequential code.
%Furthermore, our transformation, as well as these locking-based parallelization mechanisms, 
%appear to be amenable to compile-time implementation, and can thus potentially 
%lead to the development of automatic tools for efficient parallelization of sequential code,
%which is beyond the scope of the current paper. 

%% Why not read-write locks
%It is worth noting that the aforementioned mechanisms synthesize code that uses conventional symmetric locks,
%which is the type of locks handled by our transformation. We are not aware of any automatic transformation
%inserting read-write locks. We further note that read-write locks
%are more costly than conventional ones, and, moreover, threads using
%read locks contend on the shared memory locations employed by the lock's implementation~\cite{xxx}.
%In contrast, with our transformation, threads executing the optimistic phase do not contend on locks,
%and are completely invisible to other threads.

%\subsection{Evaluation}
In Section~\ref{sec:eval} we evaluate our transformation by generating an unbalanced search tree and a treap
(randomized balanced search tree).
We synthesize these data structures from \emph{sequential implementations} by applying first the algorithm of~\cite{Gueta2011} (\emph{domination locking}) to create lock-based code, and then our transformation.
%All examples are implemented in Java. 
We evaluate the scalability of the resulting code
in a range of workload scenarios on a $32$-core machine.
In all cases, the lock-based implementations do not scale --
their throughput remains flat as the number of running threads increases. In contrast, the code generated by our transformation
is scalable, and its throughput continues to grow with the number of threads.
%
We further use the Synchrobench framework~\cite{Gramoli2015} to 
compare our synthesized code to data structures that were recently hand-crafted by experts in the field~\cite{DrachslerVY2014,BronsonCCO2010,ConcurrentSkipList,EllenFRB2010,CrainGR2013}, 
as well as a state-of-the-art STM~\cite{DBLP:conf/eurosys/ShalevS06}.
%and resulted in publications in leading venues~\cite{DrachslerVY2014,BronsonCCO2010}.
Our results show that the implementations we have generated
perform comparably to custom-tailored solutions.

%
The advantage of our approach is in its \emph{generic} nature, which allows us to parallelize existing code without requiring experts to perform
manual optimizations.
Other generic approaches we are familiar with are domination locking~\cite{Gueta2011} and STM~\cite{DBLP:conf/eurosys/ShalevS06}, both of which perform worse than our transformed code in our experiments. 
%Software transactional memory is another general-purpose approach for parallelization,
%%yet due to the significant overhead associated with this approach~\cite{Cascaval:2008,DuffyTM2010}, it is often complemented
%by custom-tailored data structure implementations whose operations
%can be called from within transactions~\cite{Herlihy:2008,Koskinen:2010,NathanBronson11}.
%
Further discussion of related work appears in Section~\ref{sec:related}.

To conclude, this paper demonstrates that generic synchronization, based on a careful combination of optimism and
pessimism, is a promising approach for bringing legacy code to emerging computer architectures.
While this paper illustrates the method for tree data structures, we believe that the general direction is more broadly applicable, and maybe used with a variety of locking schemes, such as two phase locking.
Section~\ref{sec:discussion} concludes the paper and touches on some directions for future work. 