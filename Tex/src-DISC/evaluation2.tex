\section{Evaluation}
\label{sec:eval}

We evaluate the performance of our approach on 
%two types of data structures. In Section~\ref{sec:readwrite} we consider 
search trees supporting insert, delete, and get operations. 
%whereas Section~\ref{sec:range}focuses on data structures that, in addition, support range queries that retrieve all keys within a given range. 
We compare the throughput of our approach to fully pessimistic solutions applying fine-grain locking, solutions based on software transactional memory, and hand-crafted state-of-the-art data structure
implementations.
%Thesealgorithms also serve as the lock-based reference implementation at the base of our semi-optimistic implementations.We furthercompare our approach to software transactional memory and hand-crafted state-of-the-art data structure implementations supporting the same functionality.

\paragraph{Methodology} We use the micro-benchmark suite \textit{Synchrobench}~\cite{Gramoli2015}, configured as follows. 
%We follow a standard evaluation methodology
%(\cite{DrachslerVY2014,NatarajanM2014,BrownER2014,ArbelA2014}). 
Each experiment
consists of $5$ trials. A trial is a five second run in which each thread continuously executes
randomly chosen operations drawn from the workload distribution, with keys
selected uniformly at random from the range $[0,2\cdot10^6]$.
Each trial is preceded by initiating a new data structure with
%and applying an untimed pre-filling phase, which continues until the size of the data structure is within 5\% of
$10^6$ keys and a warm-up of five seconds.  Our graphs present the average throughput over all trials.
We consider three representative workloads distributions: a
\emph{read-only} workload comprised of $100\%$ lookup operations, a \emph{write-dominated}
workload consisting of insert and delete operations ($50\%$ each), and a
\emph{mixed workload} with $50\%$ lookups, $25\%$ inserts, and $25\%$
deletes.

\paragraph{Platform} All implementations are in Java. We ran the experiments on a dedicated machine with
four Intel Xeon E5-4650 processors, each with $8$ cores, for a total of $32$ threads
(with hyper-threading disabled).
We used Ubuntu 12.04.4 LTS and Java Runtime Environment (build
1.7.0\_51-b13) using the 64-Bit Server VM (build 24.51-b03, mixed mode).

%\subsection{Insert-Delete-Get Operations}
%\label{sec:readwrite}

\paragraph{Implementations}
%We start by benchmarking a search-tree supporting the basic insert,
%delete, and get (lookup) operations. 
%Our experiments evaluate unbalanced as well as balanced trees.
We start from textbook sequential implementations of an unbalanced internal binary
tree and a treap~\cite{AragonS1989}. We next  synthesize 
concurrent lock-based code by (manually) applying the domination locking technique~\cite{Gueta2011} to the sequential
data structures. The resulting algorithms are denoted \domTree and \domTreap.
Then, we manually apply our lock-removal transformation to the reference
implementations by following the algorithm line-by-line (requiring no understading of the base code)
to get our semi-optimistic versions of the code, which we call
\autoTree and \autoTreap, respectively. Note that this solution does not track the  \emph{lockedSet} for read-only operations.
Finally, we apply the optimization described in Section~\ref{sssec:alg-normal}, which eliminates explicit tracking of the \emph{lockedSet} in update operations,
and instead locks 
all objects the thread holds a pointer to in the validation phase; this optimization is applicable since our parallel implementation is synthesized using
domination locking. The resulting algorithms are denoted \optAutoTree and \optAutoTreap.   

For the competition, we parallelize the sequential implementations also using 
Deuce~\cite{Deuce2010}, a Java implementation of  TL2~\cite{DiceSS2006}. The resulting algorithms are denoted \stmTree and \stmTreap. We further compare our implementations to their hand-crafted state-of-the-art counterparts
listed in Table~\ref{table:hand-crafted}.
%\footnote{Unless described otherwise implementations are provided by Synchrobench.}. 


\begin{table}
\begin{tabular}{| l p{2.25in} |l  p{2.25in} |}
\hline 
  {\bf Unbalanced} && {\bf Balanced}  &\\  \hline 
  \textbf{\danaTree} & Locked-based unbalanced tree~\cite{DrachslerVY2014} & \textbf{\danaAVL} & Lock-based relaxed  AVL  tree~\cite{DrachslerVY2014}  \\ 
%				Drachsler et al.
  \textbf{\lockfreeTree} & Lock-free unbalanced tree~\cite{EllenFRB2010}  & \textbf{\bronson} & Lock-based relaxed  AVL tree~\cite{BronsonCCO2010} \\
   && \textbf{\friendly} & Contention-friendly tree~\cite{CrainGR2013}  \\
   && \textbf{Skiplist} & Java lock-free skiplist \\
   \hline 
\end{tabular}
\caption{Hand-crafted state-of-the-art data structures. The code of \danaTree was provided by the authors, all other implementations provided by Synchrobench.\label{table:hand-crafted}}
% ,provided by Synchrobench unless described otherwise

\end{table}



We also measured the performance of global lock-based implementations.
In all workloads, the results were identical or inferior to those
achieved by pessimistic fine-grain locking. We hence
omitted these results to avoid obscuring the presentation.



\paragraph{Results}
Figures~\ref{evaluation:results:unbalanced} and~\ref{evaluation:results:balanced}
show the throughput of unbalanced and balanced data structures, resp. We see that our semi-optimistic
solution, both optimized and unoptimized, 
 is far superior to the fully-pessimistic automated approach; it successfully overcomes the bottlenecks associated with lock contention
in  \domTree and \domTreap. 

Our approach  also outperforms STM by 1.5x to 2.5x.
The additional overhead of STM most likely stems from two reasons: deferring writes to commit time, and 
using a global clock to ensure a consistent view of the read set. The latter is done in order to satisfy opacity~\cite{GuerraouiK2008}, 
which we avoid by ``sandboxing''.
In our experiments, the code \emph{never} incurred a spurious exception or timeout due to
inconsistent reads, and so the sandboxing was not associated with a performance penalty. 

%might be due to the tradeoff between validation overhead and no internal consistency.
%To ensure opacity~\cite{GuerraouiK2008}, each time an object is read in STM, the transaction checks that the version of the lock protecting the object is valid. 
%Our algorithms read the version instead of locking the object which happens typically once during the operation.
%This is where sandboxing pays off -- we allow operations to observe inconsistent views and hence have improved performance.

Our solution comes close to custom-tailored implementations, and the optimized version is even superior to some of them.
The throughput of our read-only operations is up to 1.8x lower than that achieved by the best-in-class. 
By profiling the code, we learned that the bulk of this overhead stems from the need to track all read objects,
which is inherent to our transformation.
This is in contrast with the hand-crafted implementations, which have small overhead on reads that complete without any retries. 
In  workloads that include update operations, our solution is  up to 2.2x slower.
\Idit{Can we say that this only stems from tracking read and locked sets, because we got very few restarts? Can we say what percentage of restarts was?}


\Xomit{
The results for the read-only workload show the main overhead
of our approach. By profiling the code, we learned
that the bulk of this overhead stems from the need to track all read objects,
which is inherent to our transformation.
This is in contrast with the hand-crafted implementations,
which have small overhead on reads that complete without any retries. 

%thanks to either wait-free reads (in \danaAVL, \danaTree, \friendly and \lockfreeTree ), or optimistic validation (in \bronson).

As the ratio of updates in the workload increases, our implementation
closes this gap.
In other words, the transformed code deals well with update contention.
This might be due to the fact that once
an update phase begins, the operation is not delayed due to concurrent
read-only operations.
}

%% Eshcar: discuss small trees? the results are not the same
%We also experimented with smaller trees ($[0,2\cdot10^4]$) to test different contention levels (the results appear in Appendix~\ref{sec:appendix:results}).
%The results show that our transformation works better on larger data structures. 
%Indeed, in large data structures, update operations are more likely to operate on disjoint parts of the data, allowing high concurrency. 
%This is especially important for the automatic transformation as updates with overlapping lock sets might invalidate each other.
%; since the results showed similar trends, they are omitted here.


\begin{figure*}
\begin{center}
\input{plots/unbalanced3}
\end{center}
\caption{Throughput of unbalanced data structures.}
\label{evaluation:results:unbalanced}
\end{figure*}


\begin{figure*}
\begin{center}
\input{plots/balanced3}
\end{center}
\caption{Throughput of balanced data
structures.}
\label{evaluation:results:balanced}
\end{figure*}

\Xomit{
\subsection{Range Queries}
\label{sec:range}

Next we evaluate the performance of our approach when the data
structure supports a more intricate functionality like range queries.

Hand
crafting an implementation of a data structure that supports atomic
(linearizable) range queries is challenging.
The implementations that do support iterating through records may impose an
additional overhead on the regular read and write operations to ensure
progress of range queries.
The results in this section demonstrate that our method
allows generating a correct and efficient code, which is otherwise difficult
to obtain.

We use a skip list, which readily supports range queries by
nature of its linked-structure. The core of the implementation is the key lookup
method; once reaching the key, a key can be added or be removed in place, and
an iteration of subsequent keys can be executed by traversing
the bottom-level linked-list.

The domination locking scheme cannot be efficiently applied to the skip list
structure since it is a DAG rather than a tree. Instead, we
manually devise a pessimistic locking protocol. Our
algorithm, (inspired by the one in~\cite{HerlihyS2008}), applies
hand-over-hand locking at each level, so that at the end of the search, the
operation holds locks on two keys in each level, which define the minimal interval within this level
containing the lookup key (or the first lookup key in the case of a range query). Upon
reaching the bottom level, unnecessary locks are released, as follows: update operations only keep
locks on nodes they intend to modify, whereas
%that will be modified; while holding these locks any
%modification can be executed in isolation from other update operations.
range
queries keep the locks in the level with the minimal interval spanning the
range. Range queries then continue to use hand-over-hand locking to traverse through all keys
within the range. The use of hand-over-hand locking ensures that range queries are atomic
(linearizable), i.e., return a consistent view of the data structure.

This pessimistic lock-based algorithm is denoted \domSkiplist.
As in previous data structures, we  apply the lock-removal transformation to the
reference implementation to get a semi-optimistic algorithm, which we call
\autoSkiplist.

We also applied Deuce to the skip-list sequential implementation. The resulting algorithm is denoted \stmSkiplist.

Our approach is also compared to the aforementioned
state-of-the-art data structures that support range queries. Specifically,
we compare \autoSkiplist to 
\begin{description}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\item[\skiplist] The non-blocking Java skip-list which supports \emph{non}-linarizable range queries through iterators.
\item[\kary] A linearizable, non-blocking $k$-ary search tree
that supports range queries~\cite{BrownA12}\footnote{\url{http://www.cs.toronto.edu/~tabrown/kstrq}}.
%\item[\bronson] provides atomic range queries by traversing a clone of the
%original tree that is lazily generated
\end{description}
To ensure a fair comparison (following~\cite{BrownA12}) the range query operation in all implementations return an array of keys.
For \skiplist this means projecting a subset of the keys, iterating over them, and then copying each of these keys into an array. This does not include a snapshot, so range queries are
not always linearizable. \kary is similar to a b-tree, where the degree of the nodes is at most $k$. In our experiments we set $k$ to 64.

Like many data structure libraries, \friendly and \danaAVL
do not support atomic range queries, and
there is no straightforward way to add them.

\begin{figure*}
	\begin{center}
	\begin{{subfigure}[t]{.35\textwidth}
		\caption{Range queries}
		\input{plots/skiplists}
		\label{evaluation:results:skiplist:scans}
	\end{subfigure}
	\quad\quad
	\begin{subfigure}[t]{.35\textwidth}
		\caption{Insert and delete operations}
		\input{plots/skiplists}
		\label{evaluation:results:skiplist:updates}
	\end{subfigure}
	\ref{skiplistLegened}
	\end{center}
\caption{Small range: all threads execute either small range queries $[10,20]$
or a mix of insert and delete operations.}
\label{evaluation:results:skiplist}
\end{figure*}
}

\begin{figure*}
\begin{center}
\input{plots/smallrangeAll}
\end{center}
\caption{Small range: (on the right) read-only workload all threads execute small range queries $[10,20]$; (on the left) write-dominated workload all threads execute a mix of insert and delete operations; (in the middle) mixed workload half the threads execute small range queries (middle left)
and half the threads execute insert and delete operations (middle right).}
\label{evaluation:results:skiplist}
\end{figure*}

\begin{figure*}
	\begin{center}
	\begin{subfigure}[t]{.35\textwidth}
		\caption{Range queries}
		\input{plots/range}
		\label{evaluation:results:range}
	\end{subfigure}
	\quad\quad
	\begin{subfigure}[t]{.35\textwidth}
		\caption{Insert and delete operations}
		\input{plots/update}
		\label{evaluation:results:update}
	\end{subfigure}
	\ref{skiplistLegened}
	\end{center}
\caption{Small range: half the threads execute small range queries $[10,20]$
and half the threads execute insert and delete operations.}
\label{evaluation:results:skiplist10}
\end{figure*}


\begin{figure*}
	\begin{center}
	\begin{subfigure}[t]{.35\textwidth}
		\caption{Range queries}
		\input{plots/range1000}
		\label{evaluation:results:range1000}
	\end{subfigure}
	\quad\quad
	\begin{subfigure}[t]{.35\textwidth}
		\caption{Insert and delete operations}
		\input{plots/update1000}
		\label{evaluation:results:update1000}
	\end{subfigure}
	\ref{skiplistLegened1000}
	\end{center}
\caption{Half the threads execute large range queries $[1000,2000]$
and half the threads execute insert and delete operations.}
\label{evaluation:results:skiplist1000}
\end{figure*}

\paragraph{Results}
We start the evaluation 
%of data structures supporting range queries 
(Figure~\ref{evaluation:results:skiplist}) with the read-only workload, where all threads execute small range queries between $10$ to $20$ keys. 
%The results reflect the correlation of the implementation to scan-only workload. 
As expected, \kary has the best performance as it is optimal for batch scans. The simplicity of the non-linearizable implementation of \skiplist allows it to perform well. Our semi-optimistic transformed code, outperforms both the STM and the fully-pessimistic fine-grain transformations.

On the other extrem, we evaluated all updates (write-dominated) workloads, where the operations are a mix of insert and delete operations (50\% each). Here, splitting and merging nodes affect the performance of \kary which flattens out at 32 threads. The throughput of \autoSkiplist and \stmSkiplist are comparable, both scale nicely with the number of threads.
Again, we see that \autoSkiplist outperforms
the fully-pessimistic fine-grain one. We believe that,
as in the
domination locking versions of the tree data structures, holding a lock on the head sentinel of the skip list in
\domSkiplist, even for short periods, imposes a major performance penalty,
which is eliminated by our semi-optimistic approach. 
Finally, \autoSkiplist is also superior to the STM implementation
improving throughput by
10x. The improvement in update operations can be attributed to lack of contention on a centralized object like the global version in \stmSkiplist.

Next, we focus on a mixed workload, where half the
threads are dedicated to performing range queries, and the other half perform a
mix of insert and delete operations (50\% each).
This mix allows us to evaluate both the performance of the range queries,
and their impact on concurrent updates and vice versa.
Indeed, the results show that \kary's throughput for range query is comarable to that of \skiplist, and at 32 thread have througput similar to \autoSkiplist.

We also experimented with mixed workloads for large queries with large ranges varying between $1000$ to $2000$ keys (the results appear in Appendix~\ref{sec:appendix:results}). Here the impact of concurrent update operations on range queries is most pronounced in the results of \kary.


The evaluation of range queries focuses on a mixed workload, where half the
threads are dedicated to performing range queries, and the other half perform a
mix of insert and delete operations (50\% each).
This mix allows us to evaluate both the performance of the range queries,
and their impact on concurrent updates and vice versa.
%\eshcar{what about a workload that includes only 100\% range
%queries?}
We experiment with queries with large ranges varying between $1000$ to $2000$ keys
(Figure~\ref{evaluation:results:skiplist1000}) and small ranges
between $10$ to $20$
keys (Figure~\ref{evaluation:results:skiplist10}).

We measure the throughput of range queries and update operations separately.
The overall number of range queries executed per second is reported
in Figures~\ref{evaluation:results:range1000}
and~\ref{evaluation:results:range}, and the overall number of update (insert
and delete) operations executed per second is reported in
Figures~\ref{evaluation:results:update1000} and~\ref{evaluation:results:update}.

Again, we see that our transformed code outperforms
  to the fully-pessimistic fine-grain one. We believe that,
as in the
domination locking versions of the tree data structures, holding a lock on the head sentinel of the skip list in
\domSkiplist, even for short periods, imposes a major performance penalty,
which is eliminated by our  semi-optimistic approach.

It is superior also to the STM implementation
improving throughput by
up to three orders of magnitude -- 500x for large range queries, 2x for small
ranges, and 10x for update operations, regardless of the range size. The improvement in update operations can be attributed to lack of contention on a centralized object like the global version in \stmSkiplist. \eshcar{explain difference in range queries}


The simplicity of the non-linearizable implementation of \skiplist allows it to perform better than \autoSkiplist on range queries.  However, the performance of
\autoSkiplist is almost identical to that of \skiplist --and even better in some cases, when comparing the update operations.
%despite the fact that \skiplist performs inconsistent iterations.

Figure~\ref{evaluation:results:range1000}
shows that, for large ranges, \bronson scales well up to $8$ threads, outperforming
all other implementations, but at $32$ threads its performance deteriorates.
This might be because this implementation is
optimized for full scans or very large range queries running sequentially.
The overhead of initiating a clone of the data structure per range query, which
involves waiting for all
pending update operations to complete, hampers scalability when
a number of queries are executed in parallel. This effect is even more pronounced when the
ranges are small (Figure~\ref{evaluation:results:range}), and the throughput of
\bronson flattens out.

In addition, the lazy cloning required to support
range queries imposes an overhead on  update operations. Copying each node
during downward traversal might be the main impediment preventing these
operations from scaling when the ranges are small (Figure~\ref{evaluation:results:update}) as well as when
they are large (Figure~\ref{evaluation:results:update1000}). In contrast,
the update operations in \autoSkiplist and \skiplist do not take any special
measures for the benefit of concurrent queries (which validate themselves in
\autoSkiplist, and are not atomic in \skiplist), and hence continue to perform well in
their presence.
}
